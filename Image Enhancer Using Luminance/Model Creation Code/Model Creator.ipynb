{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Importing necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # type: ignore\n",
    "import os,keras # type: ignore\n",
    "from keras.utils import image_dataset_from_directory # type: ignore\n",
    "from tensorflow.keras.preprocessing.image import img_to_array # type: ignore\n",
    "from keras.layers import Conv2D, Add # type: ignore\n",
    "from keras.callbacks import EarlyStopping # type: ignore\n",
    "from PIL import Image # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Data Preprocesing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove (not RGB) images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_1d_images(path):\n",
    "    \n",
    "    # Change the current working directory to the specified path\n",
    "    os.chdir(path)\n",
    "    \n",
    "    # Iterate over each file in the directory\n",
    "    for filename in os.listdir():       \n",
    "\n",
    "        # Open the image using the PIL library\n",
    "        with Image.open(filename) as image:        \n",
    "            # Convert the image to a NumPy array\n",
    "            array = img_to_array(image)        \n",
    "            # Get the number of channels in the image\n",
    "            channels = array.shape[-1]     \n",
    "\n",
    "        # Check if the number of channels is not equal to 3 (not an RGB image)\n",
    "        if channels != 3:\n",
    "            # If it's not an RGB image, remove the file\n",
    "            os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove images that have a width less than 600 or a height less than 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_res_images(path, cropped_width, cropped_height):\n",
    "    \n",
    "    # Change the current working directory to the specified path\n",
    "    os.chdir(path)\n",
    "    # Get the list of files in the directory\n",
    "    file_list = os.listdir()\n",
    "    \n",
    "    # Iterate over each file in the directory\n",
    "    for filename in file_list:\n",
    "        \n",
    "        # Open the image using the PIL library\n",
    "        with Image.open(filename) as image: \n",
    "       \n",
    "            # Get the width and height of the image\n",
    "            width = image.size[0]\n",
    "            height = image.size[1]           \n",
    "\n",
    "        # Check if the width is less than 600 or height is less than 300\n",
    "        if width < cropped_width or height < cropped_height:\n",
    "            # If the image is low resolution, remove the file\n",
    "            os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training dataset using image_dataset_from_directory\n",
    "def create_datasets(Dataset_location, cropped_width, cropped_height,batch_size ,seed, validation_split):\n",
    "    train_set = image_dataset_from_directory(\n",
    "        Dataset_location,\n",
    "        image_size=(cropped_width, cropped_height),   # Resize images to the specified dimensions\n",
    "        validation_split=validation_split,            # Split 20% of the data for validation\n",
    "        subset='training',                            # Use the training subset of the data\n",
    "        seed=seed,                                    # Set a seed for reproducibility\n",
    "        batch_size=batch_size,                        # Use a batch size of 32 for training\n",
    "        label_mode=None                               # No labels are provided (unsupervised)\n",
    "    )\n",
    "\n",
    "    # Create the testing dataset using image_dataset_from_directory\n",
    "    test_set = image_dataset_from_directory(\n",
    "        Dataset_location,\n",
    "        image_size=(cropped_width, cropped_height),   # Resize images to the specified dimensions\n",
    "        validation_split=validation_split,            # Split 20% of the data for validation\n",
    "        subset='validation',                          # Use the validation subset of the data\n",
    "        seed=seed,                                    # Set a seed for reproducibility\n",
    "        batch_size=batch_size,                        # Use a batch size of 32 for testing\n",
    "        label_mode=None                               # No labels are provided (unsupervised)\n",
    "    )\n",
    "    \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(input_image):\n",
    "    input_image = input_image / 255.0\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the images from RGB into YUV, then take only the Y(Luminance) channel, and Crop image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process input features\n",
    "def process_features(input, new_width, new_height):\n",
    "    # Convert the input image to YUV color space\n",
    "    input = tf.image.rgb_to_yuv(input)   \n",
    "    # Get the index of the last axis (channels dimension)\n",
    "    last_axis = len(input.shape) - 1  \n",
    "    # Split the YUV channels\n",
    "    y, u, v = tf.split(input, 3, axis=last_axis)\n",
    "    \n",
    "    # Resize the Y channel to the specified dimensions using the area method\n",
    "    # resize the luminance (brightness) channel differently from the chrominance (color) channels to maintain certain visual characteristics.\n",
    "    return tf.image.resize(y, [new_width, new_height], method=\"area\")   \n",
    "\n",
    "# Define a function to process target (output) features\n",
    "def process_target(input):\n",
    "    # Convert the input image to YUV color space\n",
    "    input = tf.image.rgb_to_yuv(input)\n",
    "    # Get the index of the last axis (channels dimension)\n",
    "    last_axis = len(input.shape) - 1\n",
    "    # Split the YUV channels\n",
    "    y, u, v = tf.split(input, 3, axis=last_axis)\n",
    "    \n",
    "    # Return only the Y channel as the target\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_feature_with_target(train_set,test_set,cropped_width, cropped_height, upscale_factor):\n",
    "    # Calculate the input dimensions after downscaling\n",
    "    input_width = cropped_width // upscale_factor\n",
    "    input_height = cropped_height // upscale_factor\n",
    "    \n",
    "    # Apply the processing functions to the training set\n",
    "    train_set = train_set.map(lambda x: (process_features(x, input_width, input_height), process_target(x)))\n",
    "\n",
    "    # Apply the processing functions to the testing set\n",
    "    test_set = test_set.map(lambda x: (process_features(x, input_width, input_height), process_target(x)))\n",
    "\n",
    "    return train_set,test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Archetecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the RDB block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdb_block(inputs, numLayers):\n",
    "\n",
    "    # Get the number of channels in the input data\n",
    "    channels = inputs.get_shape()[-1]\n",
    "\n",
    "    # Initialize a list to store intermediate outputs in the block\n",
    "    storedOutputs = [inputs]\n",
    "\n",
    "    # Iterate through the specified number of Conv2D layers\n",
    "    for _ in range(numLayers):\n",
    "        # Concatenate the stored outputs along the channel axis\n",
    "        localConcat = tf.concat(storedOutputs, axis=-1)\n",
    "\n",
    "        # Apply a Conv2D layer with a 3x3 kernel, \"same\" padding, and ReLU activation\n",
    "        out = Conv2D(filters=channels, kernel_size=3, padding=\"same\", activation=\"relu\")(localConcat)\n",
    "\n",
    "        # Append the output of the current Conv2D layer to the list of stored outputs\n",
    "        storedOutputs.append(out)\n",
    "\n",
    "    # Concatenate all intermediate outputs along the channel axis\n",
    "    finalConcat = tf.concat(storedOutputs, axis=-1)\n",
    "\n",
    "    # Apply a pointwise Conv2D layer with a 1x1 kernel and ReLU activation\n",
    "    finalOut = Conv2D(filters=channels, kernel_size=1, padding=\"same\", activation=\"relu\")(finalConcat)\n",
    "\n",
    "    # Add the output of the pointwise Conv2D layer to the original input (residual connection)\n",
    "    finalOut = Add()([finalOut, inputs])\n",
    "\n",
    "    # Return the final output of the RDB block\n",
    "    return finalOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN layers construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(channels, upscale_factor):\n",
    "    # Input layer to accept images with shape (height, width, channels)\n",
    "    inputs = keras.Input(shape=(None, None, channels))\n",
    "\n",
    "    # Initial convolutional layers for feature extraction\n",
    "    X = Conv2D(64, 5, padding='same', activation='relu', kernel_initializer='Orthogonal')(inputs)\n",
    "\n",
    "    X = Conv2D(64, 3, padding='same', activation='relu', kernel_initializer='Orthogonal')(X)\n",
    "    # Residual Dense Block (RDB) feature extraction\n",
    "    X = rdb_block(X, numLayers=3)\n",
    "\n",
    "    # Further convolutional layers after RDB block\n",
    "    X = Conv2D(32, 3, padding='same', activation='relu', kernel_initializer='Orthogonal')(X)\n",
    "    X = rdb_block(X, numLayers=3)\n",
    "\n",
    "    # the convolutional layers after the last RDB block \n",
    "    X = Conv2D(16, 3, padding='same', activation='relu', kernel_initializer='Orthogonal')(X)\n",
    "    X = rdb_block(X, numLayers=3)\n",
    "\n",
    "    # Final convolutional layer to generate high-resolution output\n",
    "    X = Conv2D(channels * (upscale_factor**2), 3, padding='same', activation='relu', kernel_initializer='Orthogonal')(X)\n",
    "\n",
    "    # Upsample using depth_to_space operation to get the final high-resolution output\n",
    "    outputs = tf.nn.depth_to_space(X, upscale_factor)\n",
    "\n",
    "    # Create and return the Keras Model\n",
    "    return keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset Path\n",
    "Dataset_location = \"Data\"\n",
    "\n",
    "# Define the desired format for the model file name\n",
    "MoldelName = 'Super_Resolved_Model.keras'\n",
    "\n",
    "# Set the number of channels in the image\n",
    "channels = 1 #means we only pass the Y(Luminance) channel only to train the model\n",
    "\n",
    "# Define the dimensions for cropping the image and passing the hyperparameters \n",
    "cropped_width = 600\n",
    "cropped_height = 300\n",
    "batch_size=32\n",
    "seed=240\n",
    "validation_split=0.2\n",
    "\n",
    "# Set the upscale factor for the image\n",
    "upscale_factor = 3\n",
    "\n",
    "#epochs \n",
    "epochs_number = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################## Preprocess ##########################################\n",
    "remove_1d_images(Dataset_location)\n",
    "remove_low_res_images(Dataset_location, cropped_width, cropped_height)\n",
    "train_set, test_set = create_datasets(Dataset_location, cropped_width, cropped_height,batch_size ,seed, validation_split)\n",
    "train_set = train_set.map(scaling)\n",
    "test_set = test_set.map(scaling)\n",
    "train_set, test_set = mapping_feature_with_target(train_set,test_set, cropped_width, cropped_height, upscale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## Model Train ##########################################\n",
    "# Define early stopping callback to monitor training loss\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, min_delta=0.0001)\n",
    "\n",
    "# Create an instance of your defined model\n",
    "model = Model(channels, upscale_factor)        \n",
    "\n",
    "# Compile the model using Adam optimizer and Mean Squared Error (MSE) loss\n",
    "model.compile(optimizer='adam', loss='MSE')\n",
    "\n",
    "# Display the full summary of the model architecture\n",
    "model.summary(print_fn=print)\n",
    "\n",
    "# Use f-string to insert the actual number of epochs into the format\n",
    "model_file_name = MoldelName.format(epochs=epochs_number)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_set, epochs=epochs_number, callbacks=[early_stopping], validation_data=test_set)\n",
    "\n",
    "# Save the model with the dynamically generated file name\n",
    "model.save(model_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
